# MultiModal-Knowledge-Base
## What are multimodals?
Multimodal MLLMs (Multimodal Large Language Models) are advanced AI systems that combine the capabilities of Large Language Models (LLMs)—like GPT, which processes and generates text—with the ability to handle multiple types of data modalities, such as images, audio, video, and more. These models enable richer and more versatile interactions by integrating and processing data from various formats simultaneously.

Here one simple diagram from the [source](https://medium.com/@tenyks_blogger/multimodal-large-language-models-mllms-transforming-computer-vision-76d3c5dd267f)
![test3](https://github.com/pallavig702/MultiModal-Knowledge-Base/blob/main/Images/MLLM_Architecture.png)
```diff
-There are small, medium and large models.
```
![test2](https://github.com/pallavig702/MultiModal-Knowledge-Base/blob/main/Images/MLLM3.png)

```diff
-Here are examples of open source and closed source models.
```
![test](https://github.com/pallavig702/MultiModal-Knowledge-Base/blob/main/Images/MLLM_s.png)

Key Takeaways:
Small models prioritize efficiency and cost-effectiveness, often deployed in resource-constrained environments.
Medium models offer a balance between performance and resource usage, making them suitable for general enterprise needs.
Large models push the boundaries of performance, handling complex and nuanced tasks but come with significant resource and cost demands.

Few more examples of small, medium and large models.
